{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Install required packages\n",
    "# !pip install datasets\n",
    "# !pip install fasttext\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install scipy\n",
    "# !pip install tensorflow"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T17:56:43.065484Z",
     "start_time": "2024-07-12T17:56:35.306812Z"
    }
   },
   "source": [
    "# Import relevant modules\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import os\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import pearsonr"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "df = load_dataset(\"wikipedia\", \"20220301.en\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-11T18:08:27.033579Z"
    }
   },
   "source": [
    "# Preprocess the text\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s]+', ' ', text)\n",
    "    text = re.sub(r'[ \\n]+', ' ', text)\n",
    "    return text.strip().lower()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-11T18:08:27.034670Z"
    }
   },
   "source": [
    "# Select the training dataset from the loaded DataFrame\n",
    "train_dataset = df['train']\n",
    "\n",
    "# Create the file path to store the preprocessed text\n",
    "preprocessed_text_file = 'pt1.txt'\n",
    "\n",
    "# Set the counter as per the text file\n",
    "counter = 1\n",
    "\n",
    "# Open the file for writing\n",
    "with open(preprocessed_text_file, 'w', encoding='utf-8') as f:\n",
    "    # Iterate over the train dataset\n",
    "    for data_point in train_dataset:\n",
    "        text = data_point['text']\n",
    "        \n",
    "        # Perform any preprocessing steps on the text\n",
    "        preprocessed_text = preprocess(text)\n",
    "        \n",
    "        # Write the preprocessed text to the file\n",
    "        f.write(preprocessed_text + '\\n')\n",
    "        print(\"Progress: \", data_point['title'], counter)\n",
    "        if(counter == 1):\n",
    "            break\n",
    "        counter += 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "100 dimension skipgram--->"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T17:56:56.568849Z",
     "start_time": "2024-07-12T17:56:55.563959Z"
    }
   },
   "source": [
    "# Train the model (SkipGram 100 dimensions)\n",
    "model = fasttext.train_unsupervised(\"pt1.txt\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T17:57:03.862185Z",
     "start_time": "2024-07-12T17:57:02.616667Z"
    }
   },
   "source": [
    "# Save the model\n",
    "model.save_model(\"skipgram_model1.bin\")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T17:57:19.556385Z",
     "start_time": "2024-07-12T17:57:18.471806Z"
    }
   },
   "source": [
    "# Load the model\n",
    "model1 = fasttext.load_model(\"skipgram_model1.bin\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "300 dimension skipgram--->"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T19:12:59.728570Z",
     "start_time": "2024-07-11T18:54:43.729717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model (SkipGram 100 dimensions)\n",
    "model = fasttext.train_unsupervised(\"pt10000.txt\")"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T19:14:47.389635Z",
     "start_time": "2024-07-11T19:14:35.727626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "model.save_model(\"300skipgram_model10000.bin\")"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T19:14:54.111887Z",
     "start_time": "2024-07-11T19:14:53.090500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the model\n",
    "model1 = fasttext.load_model(\"300skipgram_model10000.bin\")"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T17:57:33.854910Z",
     "start_time": "2024-07-12T17:57:33.846749Z"
    }
   },
   "source": [
    "# Get the word vector for a word\n",
    "word1 = 'king'\n",
    "word2 = 'man'\n",
    "word3  = 'woman'\n",
    "\n",
    "vectorOne1 = model1.get_word_vector(word1)\n",
    "vectorTwo1 = model1.get_word_vector(word2)\n",
    "vectorThree1 = model1.get_word_vector(word3)\n",
    "\n",
    "# Calculate the cosine similarity between two words\n",
    "similarityOne1 = np.dot(vectorOne1, vectorTwo1) / (np.linalg.norm(vectorOne1) * np.linalg.norm(vectorTwo1))\n",
    "similarityTwo1 = np.dot(vectorOne1, vectorThree1) / (np.linalg.norm(vectorOne1) * np.linalg.norm(vectorThree1))\n",
    "\n",
    "# Print the cosine similarity\n",
    "print(f'The cosine similarity between \"{word1}\" and \"{word2}\" using model1 is {similarityOne1:.2f}')\n",
    "print(f'The cosine similarity between \"{word1}\" and \"{word3}\" using model1 is {similarityTwo1:.2f}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between \"king\" and \"man\" using model1 is 1.00\n",
      "The cosine similarity between \"king\" and \"woman\" using model1 is 1.00\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T17:57:36.048297Z",
     "start_time": "2024-07-12T17:57:35.867175Z"
    }
   },
   "source": [
    "# Load the STS Benchmark dataset from url and extract it\n",
    "sts_dataset = tf.keras.utils.get_file(\n",
    "    fname=\"Stsbenchmark.tar.gz\",\n",
    "    origin=\"http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\",\n",
    "    extract=True)\n",
    "\n",
    "# Load the test set of the STS Benchmark dataset into a pandas DataFrame\n",
    "sts_test = pandas.read_table(\n",
    "    os.path.join(\n",
    "        os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-test.csv\"),\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    skip_blank_lines=True,\n",
    "    usecols=[4, 5, 6], #similarity scores and sentence pairs\n",
    "    names=[\"sim\", \"sent_1\", \"sent_2\"])\n",
    "\n",
    "# Cleanup some NaN values in sts_tests\n",
    "sts_test = sts_test[[isinstance(s, str) for s in sts_test['sent_2']]]# Keep rows where 'sent_2' is a string"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T17:57:38.186659Z",
     "start_time": "2024-07-12T17:57:37.635909Z"
    }
   },
   "source": [
    "# Preprocess a sentence by converting it to lowercase and splitting into words\n",
    "def preprocess_sentence(sentence):\n",
    "  return sentence.lower().split()\n",
    "\n",
    "# Calculate the sentence embedding using average word embeddings\n",
    "def get_sentence_embedding(model, sentence):\n",
    "  return model.get_sentence_vector(sentence)\n",
    "\n",
    "# Evaluate the FastText model on the STS Benchmark data\n",
    "def run_sts_benchmark(model, sts_data):\n",
    "  scores = []   # empty list to store similarity scores for each sentence pair\n",
    "  for sent1, sent2, label in zip(sts_data[\"sent_1\"], sts_data[\"sent_2\"], sts_data[\"sim\"]):\n",
    "    # Preprocess sentences\n",
    "    preprocessed_sent1 = preprocess_sentence(sent1)\n",
    "    preprocessed_sent2 = preprocess_sentence(sent2)\n",
    "\n",
    "    # Get sentence embeddings one at a time\n",
    "    sentence1_embedding = get_sentence_embedding(model, \" \".join(preprocessed_sent1))\n",
    "    sentence2_embedding = get_sentence_embedding(model, \" \".join(preprocessed_sent2))\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = 1 - cosine(sentence1_embedding, sentence2_embedding)  # Higher value means more similar\n",
    "      \n",
    "    scores.append(cosine_similarity)\n",
    "  return scores\n",
    "\n",
    "\n",
    "\n",
    "# Load STS Benchmark data\n",
    "sts_data = sts_test\n",
    "\n",
    "# Run STS Benchmark evaluation\n",
    "scores = run_sts_benchmark(model1, sts_data)\n",
    "\n",
    "# Calculate Pearson correlation coefficient\n",
    "pearson_correlation, pValue = pearsonr(scores, sts_data[\"sim\"])\n",
    "\n",
    "# Print the results\n",
    "print('Pearson correlation coefficient = {0}\\np-value = {1}'.format(pearson_correlation, pValue))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient = 0.12380486264892637\n",
      "p-value = 4.006196895922756e-06\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
